<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PonderTTT: When to Ponder</title>
    <link rel="stylesheet" href="styles.css">
    <meta name="description"
        content="Adaptive Test-Time Training for code language modeling. Training-free Reconstruction Gating achieves 82-89% Oracle Recovery.">
    <!-- KaTeX for LaTeX rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.css"
        integrity="sha384-WcoG4HRXMzYzfCgiyfrySxx90XSl2rxY5mnVY5TwtWE6KLrArNKn0T/mOgNL0Mmi" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.js"
        integrity="sha384-J+9dG2KMoiR9hqcFao0IBLwxt6zpcyN68IgwzsCSkbreXUjmNVRhPFTssqdSGjwQ"
        crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/contrib/auto-render.min.js"
        integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
</head>

<body>

    <div class="container">
        <header>
            <h1>PonderTTT</h1>
            <p class="subtitle">When to Ponder: Adaptive Compute Allocation for Code Language Modeling via Test-Time
                Training</p>
            <div class="links">
                <a href="https://github.com/deveworld/ponderttt" class="btn">View on GitHub</a>
                <a href="PonderTTT_v1_Preprint.pdf" class="btn btn-outline">Read Paper (PDF)</a>
            </div>
        </header>

        <section id="core-idea">
            <h2>Core Idea: Training-Free Reconstruction Gating</h2>
            <p>PonderTTT introduces <strong>Adaptive Test-Time Training (TTT)</strong> with a training-free gating
                mechanism:
                the TTT layer's self-supervised <strong>Reconstruction Loss</strong> selectively triggers updates.
                No learned classifier or auxiliary networks required—only a threshold calibrated on unlabeled data.
            </p>

            <h3>Key Results: 82–89% Oracle Recovery</h3>
            <p>Using Reconstruction Gating with EMA-based threshold adaptation:</p>
            <table>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>SKIP (Base)</th>
                        <th>Oracle</th>
                        <th>Ours</th>
                        <th>Recovery</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Small (124M)</strong></td>
                        <td>2.324</td>
                        <td>1.935</td>
                        <td><strong>1.977</strong></td>
                        <td>89.2%</td>
                    </tr>
                    <tr>
                        <td><strong>Medium (355M)</strong></td>
                        <td>1.909</td>
                        <td>1.653</td>
                        <td><strong>1.697</strong></td>
                        <td>82.8%</td>
                    </tr>
                    <tr>
                        <td><strong>Large (774M)</strong></td>
                        <td>2.005</td>
                        <td>1.580</td>
                        <td><strong>1.656</strong></td>
                        <td>82.1%</td>
                    </tr>
                    <tr>
                        <td><strong>XL (1.5B)</strong></td>
                        <td>1.875</td>
                        <td>1.518</td>
                        <td><strong>1.576</strong></td>
                        <td>83.8%</td>
                    </tr>
                </tbody>
            </table>
            <p><strong>Key Finding:</strong> Training-free Reconstruction Gating achieves <strong>82–89% Oracle
                    Recovery</strong>
                while significantly outperforming Random Skip baselines (1–3% lower loss on Python, up to 16% on OOD
                languages).</p>

            <h3>Correlation Analysis</h3>
            <table>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Language</th>
                        <th>Correlation ($r$)</th>
                        <th>Oracle Recovery</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Small (124M)</strong></td>
                        <td>Python</td>
                        <td><strong>+0.84</strong></td>
                        <td>89.2%</td>
                    </tr>
                    <tr>
                        <td><strong>Medium (355M)</strong></td>
                        <td>Python</td>
                        <td>+0.43</td>
                        <td>82.8%</td>
                    </tr>
                    <tr>
                        <td><strong>Large (774M)</strong></td>
                        <td>Python</td>
                        <td>+0.62</td>
                        <td>82.1%</td>
                    </tr>
                    <tr>
                        <td><strong>XL (1.5B)</strong></td>
                        <td>Python</td>
                        <td>+0.61</td>
                        <td>83.8%</td>
                    </tr>
                    <tr>
                        <td><strong>XL (1.5B)</strong></td>
                        <td>JavaScript (OOD)</td>
                        <td>+0.74</td>
                        <td>—</td>
                    </tr>
                    <tr>
                        <td><strong>XL (1.5B)</strong></td>
                        <td>Java (OOD)</td>
                        <td><strong>+0.84</strong></td>
                        <td>—</td>
                    </tr>
                    <tr>
                        <td><strong>XL (1.5B)</strong></td>
                        <td>Go (OOD)</td>
                        <td>+0.58</td>
                        <td>—</td>
                    </tr>
                </tbody>
            </table>

            <h3>OOD Generalization (XL 1.5B)</h3>
            <table>
                <thead>
                    <tr>
                        <th>Language</th>
                        <th>SKIP</th>
                        <th>Random</th>
                        <th>Ours</th>
                        <th>Oracle</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>JavaScript</td>
                        <td>2.85</td>
                        <td>2.12</td>
                        <td><strong>1.84</strong></td>
                        <td>1.57</td>
                    </tr>
                    <tr>
                        <td>Java</td>
                        <td>3.21</td>
                        <td>2.33</td>
                        <td><strong>1.95</strong></td>
                        <td>1.64</td>
                    </tr>
                    <tr>
                        <td>Go</td>
                        <td>6.52</td>
                        <td>4.25</td>
                        <td><strong>4.15</strong></td>
                        <td>3.70</td>
                    </tr>
                </tbody>
            </table>
            <p><strong>Up to 16% lower loss</strong> on OOD languages compared to Random Skip.</p>

            <h3>Decision Accuracy</h3>
            <table>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Random Skip</th>
                        <th>Ours (Recon Gating)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Small (124M)</td>
                        <td>52.0%</td>
                        <td><strong>59.1%</strong></td>
                    </tr>
                    <tr>
                        <td>Medium (355M)</td>
                        <td>52.2%</td>
                        <td><strong>57.5%</strong></td>
                    </tr>
                    <tr>
                        <td>Large (774M)</td>
                        <td>51.8%</td>
                        <td><strong>59.2%</strong></td>
                    </tr>
                    <tr>
                        <td>XL (1.5B)</td>
                        <td>52.8%</td>
                        <td><strong>59.6%</strong></td>
                    </tr>
                </tbody>
            </table>
            <p>~59% decision accuracy vs Oracle (7pp above random), statistically significant ($p < 10^{-10}$, McNemar's
                    test).</p>

        </section>

        <section id="technical-architecture">
            <h2>Technical Architecture</h2>
            <p>Pure <strong>JAX/Flax NNX</strong> implementation with multi-scale model support.</p>

            <h3>Supported Models</h3>
            <table>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Parameters</th>
                        <th>Status</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>GPT-2 Small</td>
                        <td>124M</td>
                        <td>✅ Validated</td>
                    </tr>
                    <tr>
                        <td>GPT-2 Medium</td>
                        <td>355M</td>
                        <td>✅ Validated</td>
                    </tr>
                    <tr>
                        <td>GPT-2 Large</td>
                        <td>774M</td>
                        <td>✅ Validated</td>
                    </tr>
                    <tr>
                        <td>GPT-2 XL</td>
                        <td>1.5B</td>
                        <td>✅ Validated</td>
                    </tr>
                    <tr>
                        <td>Gemma 3 4B</td>
                        <td>4B</td>
                        <td>Planned (v2)</td>
                    </tr>
                    <tr>
                        <td>Gemma 3 12B</td>
                        <td>12B</td>
                        <td>Planned (v2)</td>
                    </tr>
                </tbody>
            </table>

            <h3>Components</h3>
            <ul>
                <li><strong>Base Model:</strong> Pretrained backbone with frozen weights</li>
                <li><strong>TTT Layer:</strong> Fast-weight adapter with self-supervised updates</li>
                <li><strong>Gating:</strong> Training-free threshold on reconstruction loss with EMA adaptation</li>
            </ul>

            <h3>Loss Function</h3>
            <p>\( L_{total} = L_{CE} + \beta \cdot L_{TTT} \) where $\beta = 0.1$</p>
            <ul>
                <li>\( L_{CE} \): Main task cross-entropy (next-token prediction)</li>
                <li>\( L_{TTT} \): TTT reconstruction loss (self-supervised, also used for gating)</li>
            </ul>
        </section>

        <section id="quick-start">
            <h2>Quick Start</h2>

            <h3>Installation</h3>
            <pre><code># Install uv if you do not have it yet
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install the project
uv pip install -e . --group gpu  # or tpu/cpu</code></pre>

            <h3>Reproduce Paper Results</h3>
            <pre><code>chmod +x scripts/run_all_experiments.sh
./scripts/run_all_experiments.sh</code></pre>

        </section>

        <section id="citation">
            <h2>Citation</h2>
            <div class="citation">@article{sim2025ponderttt,
                title={When to Ponder: Adaptive Compute Allocation for Code Generation via Test-Time Training},
                author={Sim, Gihyeon},
                year={2025}
                }</div>
        </section>

        <footer>
            <p>&copy; 2025 PonderTTT Project. Built with JAX/Flax NNX.</p>
        </footer>
    </div>

</body>

</html>
</code></pre>