<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PonderTTT: Adaptive Test-Time Training</title>
    <link rel="stylesheet" href="styles.css">
    <meta name="description"
        content="Adaptive, budget-aware Test-Time Training (TTT) for code generation models. Reconstruction Gating recovers 99% of oracle performance.">
    <!-- KaTeX for LaTeX rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.css"
        integrity="sha384-WcoG4HRXMzYzfCgiyfrySxx90XSl2rxY5mnVY5TwtWE6KLrArNKn0T/mOgNL0Mmi" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.js"
        integrity="sha384-J+9dG2KMoiR9hqcFao0IBLwxt6zpcyN68IgwzsCSkbreXUjmNVRhPFTssqdSGjwQ"
        crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/contrib/auto-render.min.js"
        integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
</head>

<body>

    <div class="container">
        <header>
            <h1>PonderTTT</h1>
            <p class="subtitle">Adaptive, budget-aware Test-Time Training (TTT) for code generation models built with
                JAX/Flax NNX.</p>
            <div class="links">
                <a href="https://github.com/deveworld/ponderttt" class="btn">View on GitHub</a>
                <a href="PonderTTT_preprint.pdf" class="btn btn-outline">Read Preprint</a>
            </div>
        </header>

        <section id="core-idea">
            <h2>Core Idea: Reconstruction Gating</h2>
            <p>PonderTTT introduces <strong>Adaptive Test-Time Training</strong> with a simple heuristic: the
                TTT layer's self-supervised <strong>Reconstruction Loss</strong>. We investigated this signal as
                an inference-compatible proxy for learning benefit.
            </p>

            <h3>Scale-Dependent Reliability: XL Models Recover >97% of Oracle</h3>
            <p>Using the full-sequence reconstruction loss (<code>ttt_loss_init</code>), we observe that gating signal
                reliability improves with model scale:</p>
            <table>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Language</th>
                        <th>Correlation ($r$)</th>
                        <th>Oracle Recovery</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>125M</strong></td>
                        <td>Python</td>
                        <td><strong>+0.67</strong></td>
                        <td>89.5%</td>
                    </tr>
                    <tr>
                        <td><strong>XL (1.5B)</strong></td>
                        <td>Python</td>
                        <td><strong>+0.76</strong></td>
                        <td>97.3%</td>
                    </tr>
                    <tr>
                        <td><strong>XL (1.5B)</strong></td>
                        <td>JavaScript (OOD)</td>
                        <td><strong>+0.84</strong></td>
                        <td>96.7%</td>
                    </tr>
                    <tr>
                        <td><strong>XL (1.5B)</strong></td>
                        <td>Java (OOD)</td>
                        <td><strong>+0.90</strong></td>
                        <td>97.5%</td>
                    </tr>
                    <tr>
                        <td><strong>XL (1.5B)</strong></td>
                        <td>Go (OOD)</td>
                        <td><strong>+0.75</strong></td>
                        <td>97.7%</td>
                    </tr>
                </tbody>
            </table>

            <p><strong>Key Finding:</strong> The reliability of the self-supervised gating signal <strong>improves with
                    model scale</strong>. While 125M shows moderate correlation ($r \approx 0.67$), XL models
                demonstrate strong correlation ($r \ge 0.75$), enabling Reconstruction Gating to recover <strong>96â€“97%
                    of Oracle performance</strong> across all evaluated languages.</p>

            <h3>Key Results (In-Distribution Python)</h3>
            <table>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Base Loss</th>
                        <th>Ours Loss</th>
                        <th>Oracle Recovery</th>
                        <th>Cost</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>125M</strong></td>
                        <td>2.332</td>
                        <td><strong>1.990</strong></td>
                        <td>89.5%</td>
                        <td>2.0x</td>
                    </tr>
                    <tr>
                        <td><strong>350M</strong></td>
                        <td>1.936</td>
                        <td><strong>1.711</strong></td>
                        <td>88.2%</td>
                        <td>2.0x</td>
                    </tr>
                    <tr>
                        <td><strong>1B (Large)</strong></td>
                        <td>2.005</td>
                        <td><strong>1.738</strong></td>
                        <td>97.1%</td>
                        <td>2.0x</td>
                    </tr>
                    <tr>
                        <td><strong>XL (1.5B)</strong></td>
                        <td>1.875</td>
                        <td><strong>1.622</strong></td>
                        <td>97.3%</td>
                        <td>2.0x</td>
                    </tr>
                </tbody>
            </table>
            <p><strong>Scale Advantage:</strong> Larger models (1B, XL) achieve near-perfect Oracle recovery (>97%),
                demonstrating that the gating signal becomes more reliable at scale.</p>

            <h3>OOD Generalization (Trained on Python, Tested on New Languages)</h3>
            <table>
                <thead>
                    <tr>
                        <th>Language</th>
                        <th>Scale</th>
                        <th>Baseline (SKIP)</th>
                        <th>Ours (Gating)</th>
                        <th>Oracle Recovery</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>JavaScript</td>
                        <td>125M</td>
                        <td>2.640</td>
                        <td>2.158</td>
                        <td>92.5%</td>
                    </tr>
                    <tr>
                        <td>JavaScript</td>
                        <td>XL</td>
                        <td>2.852</td>
                        <td>2.138</td>
                        <td>96.7%</td>
                    </tr>
                    <tr>
                        <td>Java</td>
                        <td>125M</td>
                        <td>3.042</td>
                        <td>2.287</td>
                        <td>91.9%</td>
                    </tr>
                    <tr>
                        <td>Java</td>
                        <td>XL</td>
                        <td>3.213</td>
                        <td>2.292</td>
                        <td>97.5%</td>
                    </tr>
                    <tr>
                        <td>Go</td>
                        <td>125M</td>
                        <td>7.944</td>
                        <td>5.047</td>
                        <td>93.1%</td>
                    </tr>
                    <tr>
                        <td>Go</td>
                        <td>XL</td>
                        <td>6.520</td>
                        <td>4.275</td>
                        <td>97.7%</td>
                    </tr>
                </tbody>
            </table>

            <h3>Latency & Efficiency (NVIDIA A100)</h3>
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Latency</th>
                        <th>Rel. Speed</th>
                        <th>GPU Util</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Baseline (SKIP)</td>
                        <td>2.76 ms</td>
                        <td>1.00x</td>
                        <td>11.4%</td>
                    </tr>
                    <tr>
                        <td>Baseline (UPDATE_1)</td>
                        <td>2.49 ms</td>
                        <td>0.90x</td>
                        <td>23.3%</td>
                    </tr>
                    <tr>
                        <td><strong>PonderTTT</strong></td>
                        <td><strong>2.94 ms</strong></td>
                        <td><strong>1.07x</strong></td>
                        <td><strong>23.1%</strong></td>
                    </tr>
                </tbody>
            </table>
            <p><strong>Memory-Bound Efficiency:</strong> TTT updates utilize idle compute capacity (GPU utilization 11%
                &rarr; 23%) in memory-bound small-batch inference, incurring minimal latency overhead.</p>
        </section>

        <section id="technical-architecture">
            <h2>Technical Architecture</h2>
            <p>Pure <strong>JAX/Flax NNX</strong> implementation with multi-scale model support.</p>

            <h3>Supported Models</h3>
            <table>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Parameters</th>
                        <th>Status</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>GPT-2 125M</td>
                        <td>125M</td>
                        <td>Validated</td>
                    </tr>
                    <tr>
                        <td>GPT-2 350M</td>
                        <td>350M</td>
                        <td>Validated</td>
                    </tr>
                    <tr>
                        <td>GPT-2 Large</td>
                        <td>774M</td>
                        <td>Validated</td>
                    </tr>
                    <tr>
                        <td>GPT-2 XL</td>
                        <td>1.5B</td>
                        <td>Validated</td>
                    </tr>
                    <tr>
                        <td>Gemma 3 1B</td>
                        <td>1B</td>
                        <td>In Progress</td>
                    </tr>
                    <tr>
                        <td>Gemma 3 4B</td>
                        <td>4B</td>
                        <td>In Progress</td>
                    </tr>
                    <tr>
                        <td>Gemma 3 12B</td>
                        <td>12B</td>
                        <td>In Progress (TPU)</td>
                    </tr>
                </tbody>
            </table>

            <h3>Components</h3>
            <ul>
                <li><strong>Base Model:</strong> Pretrained backbone with frozen weights</li>
                <li><strong>TTT Layer:</strong> Fast-weight adapter with self-supervised updates</li>
                <li><strong>Gating Signals:</strong>
                    <ul>
                        <li>Reconstruction Loss (threshold-based)</li>
                        <li>Budget-aware Thresholds</li>
                    </ul>
                </li>
            </ul>

            <h3>Loss Function</h3>
            <p>\( L_{total} = L_{CE} + \beta \cdot L_{TTT} \)</p>
            <ul>
                <li>\( L_{CE} \): Main task cross-entropy (next-token prediction)</li>
                <li>\( L_{TTT} \): TTT reconstruction loss (self-supervised adaptation signal)</li>
            </ul>
        </section>

        <section id="roadmap">
            <h2>Roadmap & Status</h2>

            <h3>Phase 1: Complete (Preprint)</h3>
            <ul>
                <li>Pure NNX GPT-2, TTT Layer implementation</li>
                <li>Gumbel-Softmax training for SKIP/UPDATE decisions</li>
                <li>End-to-End differentiable training with budget constraints</li>
                <li>Results on GPT-2 (125M, 350M) with OOD evaluation</li>
            </ul>

            <h3>Phase 2: In Progress</h3>
            <table>
                <thead>
                    <tr>
                        <th>Component</th>
                        <th>Status</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Crawl Phase (TTT Improvement gating)</td>
                        <td>Complete</td>
                    </tr>
                    <tr>
                        <td>Walk Phase (Fixed threshold, online)</td>
                        <td>Complete</td>
                    </tr>
                    <tr>
                        <td>Run Phase (Multi-signal + budget-aware)</td>
                        <td>In Progress</td>
                    </tr>
                    <tr>
                        <td>Gemma 3 Integration (1B, 4B, 12B)</td>
                        <td>In Progress</td>
                    </tr>
                    <tr>
                        <td>TPU Pod Sharding</td>
                        <td>In Progress</td>
                    </tr>
                    <tr>
                        <td>LoRA-TTT</td>
                        <td>Planned</td>
                    </tr>
                    <tr>
                        <td>Reasoning Benchmarks (MATH500, GSM8K, etc.)</td>
                        <td>Planned</td>
                    </tr>
                </tbody>
            </table>
            <p>See <code>paper/plan.md</code> for detailed roadmap.</p>
        </section>

        <section id="quick-start">
            <h2>Quick Start</h2>

            <h3>Installation</h3>
            <pre><code># Install uv if you do not have it yet
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install the project
uv pip install -e . --group gpu  # or tpu/cpu</code></pre>

            <h3>Training-Free Gating (Recommended)</h3>
            <pre><code>from ponderttt.models import create_advanced_gating

# Create gating network
gating = create_advanced_gating(
    mode="threshold",  # or "budget_aware", "learned"
    use_entropy=True,
    use_token_confidence=True,
    target_update_rate=0.5,
)

# Get gating decision
result = gating(
    ttt_improvement=ttt_stats["ttt_loss_step_0"] - ttt_stats["ttt_loss_step_1"],
    logits=model_output["logits"],
)
should_update = result["decision"]</code></pre>

            <h3>Reproduce Paper Results</h3>
            <pre><code>chmod +x scripts/run_all_experiments.sh
./scripts/run_all_experiments.sh</code></pre>

            <h3>Evaluate Gating Methods</h3>
            <pre><code># Compare TTT-only vs Multi-signal vs Budget-aware
python -m ponderttt.experiments.evaluate_advanced_gating \
    --checkpoint outputs/baselines/125m_update1/checkpoints/checkpoint_100000/ \
    --num_batches 1000</code></pre>

            <h3>Gemma 3 on TPU</h3>
            <pre><code>from ponderttt.models.gemma3 import (
    Gemma3Config,
    Gemma3TTTModel,
    load_gemma3_from_huggingface,
    create_device_mesh,
    ShardingConfig,
)

# Initialize
config = Gemma3Config.gemma3_4b()
model = Gemma3TTTModel(config, ttt_config, rngs=rngs)

# Load pretrained weights
model = load_gemma3_from_huggingface(model, "google/gemma-3-4b-pt")

# Setup TPU sharding
mesh = create_device_mesh(ShardingConfig())</code></pre>
        </section>

        <section id="citation">
            <h2>Citation</h2>
            <div class="citation">@article{sim2025ponderttt,
                title={Learning to Ponder: Adaptive Compute Allocation via Test-Time Training},
                author={Sim, Gihyeon},
                year={2025}
                }</div>
        </section>

        <footer>
            <p>&copy; 2025 PonderTTT Project. Built with JAX/Flax NNX.</p>
        </footer>
    </div>

</body>

</html>